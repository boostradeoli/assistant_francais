# ==========================================
# ğŸš€ AGENT IA AVEC RÃ‰FLEXION & VALIDATION
# ==========================================

import logging
import time
import threading
import pyttsx3
import speech_recognition as sr
import ollama  # ğŸ“Œ Utilisation d'Ollama pour interagir avec le LLM

from services.email import email_module
from services.visio import visio_module
from services.photo import photo_module
from services.printing import printing_module

# ==========================================
# ğŸ”¹ INITIALISATION
# ==========================================

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

tts_engine = pyttsx3.init()
tts_engine.setProperty("rate", 150)
tts_engine.setProperty("volume", 1.0)

recognizer = sr.Recognizer()
speech_lock = threading.Lock()

# ğŸ”¹ ModÃ¨le Ollama utilisÃ© (assure-toi qu'il est bien installÃ©)
LLM_MODEL_NAME = "tinyllama"

# ==========================================
# ğŸ”¹ FONCTIONS UTILITAIRES
# ==========================================

def speak(text: str):
    """ ğŸ”Š Lâ€™agent IA parle et attend la fin avant de continuer. """
    global tts_engine
    with speech_lock:
        logging.info(f"ğŸ—£ï¸ Agent IA dit : {text}")

        # ğŸ”´ RÃ©initialisation du moteur pour Ã©viter les blocages
        tts_engine = pyttsx3.init()
        tts_engine.setProperty("rate", 150)
        tts_engine.setProperty("volume", 1.0)

        tts_engine.say(text)
        tts_engine.runAndWait()
        time.sleep(1)

def listen() -> str:
    """ ğŸ¤ Ã‰coute et retourne le texte transcrit. """
    global recognizer
    with speech_lock:
        with sr.Microphone() as source:
            logging.info("ğŸ¤ Ã‰coute...")
            try:
                recognizer.adjust_for_ambient_noise(source, duration=1)
                audio = recognizer.listen(source, timeout=5)
            except sr.WaitTimeoutError:
                logging.warning("â³ Aucun son dÃ©tectÃ©.")
                return ""

    try:
        text = recognizer.recognize_google(audio, language="fr-FR")
        logging.info(f"âœ… Reconnu : {text}")
        return text.lower()
    except sr.UnknownValueError:
        logging.warning("ğŸ”‡ Impossible de comprendre.")
        return ""
    except sr.RequestError as e:
        logging.error(f"âŒ Erreur de reconnaissance vocale : {e}")
        return ""

# ==========================================
# ğŸ”¹ INTERACTION AVEC LE LLM (OLLAMA)
# ==========================================

def query_llm(text: str) -> dict:
    """ ğŸ§  Envoie la requÃªte au modÃ¨le Ollama et rÃ©cupÃ¨re la rÃ©ponse. """
    try:
        response = ollama.generate(
            model=LLM_MODEL_NAME,
            prompt=f"""Tu es un assistant qui aide les utilisateurs Ã  accomplir des tÃ¢ches. 
            Donne une rÃ©ponse claire et directe en un paragraphe court. 
            Identifie l'intention parmi : email, visio, photo, imprimer. 
            Si aucune intention claire n'est dÃ©tectÃ©e, rÃ©ponds normalement.
            
            Utilisateur : {text}
            Assistant :"""
        )

        response_text = response.get("response", "").strip()
        logging.info(f"ğŸ¤– RÃ©ponse du LLM : {response_text}")

        # ğŸ”¹ DÃ©tection des intentions avec des mots-clÃ©s
        if any(word in response_text.lower() for word in ["email", "mail"]):
            return {"intention": "email", "response": response_text}
        elif any(word in response_text.lower() for word in ["visioconfÃ©rence", "appel vidÃ©o"]):
            return {"intention": "visio", "response": response_text}
        elif "photo" in response_text.lower():
            return {"intention": "photo", "response": response_text}
        elif any(word in response_text.lower() for word in ["impression", "imprimer"]):
            return {"intention": "imprimer", "response": response_text}
        else:
            return {"intention": "unknown", "response": response_text}

    except Exception as e:
        logging.error(f"âŒ Erreur avec Ollama : {e}")
        return {"intention": "unknown", "response": "Je ne peux pas traiter cette requÃªte pour le moment."}

def verify_response(response_text: str) -> str:
    """ ğŸ§ VÃ©rifie la rÃ©ponse gÃ©nÃ©rÃ©e pour s'assurer qu'elle est correcte. """
    try:
        response = ollama.generate(
            model=LLM_MODEL_NAME,
            prompt=f"Peux-tu vÃ©rifier et corriger cette rÃ©ponse si nÃ©cessaire ? {response_text}"
        )

        validated_response = response.get("response", "").strip()
        logging.info(f"âœ… RÃ©ponse validÃ©e : {validated_response}")

        return validated_response if validated_response else response_text

    except Exception as e:
        logging.error(f"âŒ Erreur de validation avec Ollama : {e}")
        return response_text

# ==========================================
# ğŸ”¹ EXÃ‰CUTION DES COMMANDES
# ==========================================

def execute_command(intent: str):
    """ âš¡ ExÃ©cute l'action correspondant Ã  l'intention dÃ©tectÃ©e. """
    logging.info(f"ğŸš€ ExÃ©cution de l'action pour l'intention : {intent}")

    if intent == "email":
        speak("J'ouvre votre messagerie pour Ã©crire un email.")
        time.sleep(1)
        email_module.compose_email()
    elif intent == "visio":
        speak("Je dÃ©marre la visioconfÃ©rence.")
        time.sleep(1)
        visio_module.start_visio()
    elif intent == "photo":
        speak("J'ouvre votre dossier de photos.")
        time.sleep(1)
        photo_module.view_photos()
    elif intent == "imprimer":
        speak("Je prÃ©pare l'impression du document.")
        time.sleep(1)
        printing_module.print_document()
    else:
        speak("DÃ©solÃ©, cette action nâ€™est pas encore supportÃ©e.")

# ==========================================
# ğŸ”¹ BOUCLE PRINCIPALE
# ==========================================

def run_agent():
    """ ğŸ”„ Agent IA : Ã‰coute â†’ Envoie au LLM â†’ VÃ©rifie â†’ RÃ©pond â†’ Agit. """
    speak("Bonjour, comment puis-je vous aider ?")

    while True:
        user_input = listen()
        if not user_input:
            speak("Je ne vous ai pas bien entendu. Pouvez-vous rÃ©pÃ©ter ?")
            continue

        # ğŸ”¹ Envoie au LLM principal
        llm_response = query_llm(user_input)
        intent = llm_response["intention"]
        response_text = llm_response["response"]

        # ğŸ”¹ VÃ©rification de la rÃ©ponse
        validated_response = verify_response(response_text)

        # ğŸ”¹ L'agent IA parle uniquement aprÃ¨s validation
        speak(validated_response)

        if intent != "unknown":
            execute_command(intent)

        # ğŸ”¹ VÃ©rifie si l'utilisateur veut continuer
        speak("Voulez-vous faire autre chose ?")
        continue_input = listen()
        if "non" in continue_input:
            speak("D'accord, Ã  bientÃ´t !")
            break

# ==========================================
# ğŸ”¹ LANCEMENT
# ==========================================

if __name__ == "__main__":
    run_agent()
